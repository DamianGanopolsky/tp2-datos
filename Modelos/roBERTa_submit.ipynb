{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "colab_type": "code",
    "id": "WXExTMpJI6GX",
    "outputId": "3f406037-75a5-460e-c261-afc603bdd901"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1MB 3.4MB/s \n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.91\n",
      "Collecting transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
      "\u001b[K     |████████████▍                   | 296kB 3.4MB/s eta 0:00:01"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WuZTuSlG5sy5"
   },
   "source": [
    "Importamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lznNXie3I95i"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import util\n",
    "from process_text import generate_df\n",
    "import gc\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Flatten, MaxPooling1D, Embedding, GlobalMaxPooling1D, Dropout, Input\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "from transformers import RobertaTokenizer, RobertaConfig, TFRobertaPreTrainedModel\n",
    "from transformers.modeling_tf_roberta import TFRobertaMainLayer\n",
    "from transformers.modeling_tf_utils import get_initializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s_bs-CUb5nZZ"
   },
   "source": [
    "Cargamos los datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SWVqCjeII-FZ"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv', dtype={'id': np.int16, 'target': np.int8})\n",
    "test_df = pd.read_csv('test.csv', dtype={'id': np.int16, 'target': np.int8})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cambio el dataframe y obtengo uno con los targets de textos repetidos asignados en teoria bien.\n",
    "No ejecutar esta linea si no se busca cambiar los targets de textos repetidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_fixed=generate_df(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DmhRwYYl5gsN"
   },
   "source": [
    "Activamos el uso de TPU en tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "--BkDb3SMy-G"
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "print(\"Tensorflow version \" + tf.__version__)\n",
    "\n",
    "try:\n",
    "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
    "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
    "except ValueError:\n",
    "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
    "\n",
    "tf.config.experimental_connect_to_cluster(tpu)\n",
    "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "34pNaXr65yI5"
   },
   "source": [
    "Definimos roBERTa usando la librearia transformers de huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ivnVNQ5YJCoq"
   },
   "outputs": [],
   "source": [
    "class roBERTaModel(TFRobertaPreTrainedModel):\n",
    "\n",
    "    def __init__(self, config, *inputs, **kwargs):\n",
    "        super(roBERTaModel, self).__init__(config, *inputs, **kwargs)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.roberta = TFRobertaMainLayer(config, name=\"roberta\")\n",
    "        self.dropout_1 = tf.keras.layers.Dropout(0.3)\n",
    "        self.classifier = tf.keras.layers.Dense(units=config.num_labels, name='classifier', kernel_initializer=get_initializer(config.initializer_range))\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        outputs = self.roberta(inputs, **kwargs)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout_1(pooled_output, training=kwargs.get('training', False))\n",
    "        logits = self.classifier(pooled_output)\n",
    "        outputs = (logits,) + outputs[2:]\n",
    "\n",
    "        return outputs\n",
    "\n",
    "class roBERTaClassifier():\n",
    "    \n",
    "    def __init__(self, max_seq_length, lr, epochs, batch_size, splits):\n",
    "        self.model_name = 'roberta-base'\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained(self.model_name)\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.splits = splits\n",
    "        \n",
    "        \n",
    "    def encode(self, text_column):\n",
    "\n",
    "      def tokenize(x):\n",
    "        return self.tokenizer.encode_plus(x, max_length=self.max_seq_length, pad_to_max_length=True, truncation=True)\n",
    "\n",
    "      output = text_column.apply(lambda x: tokenize(x))\n",
    "      input_ids = np.array([feature['input_ids'] for feature in output])\n",
    "      masks = np.array([feature['attention_mask'] for feature in output])\n",
    "\n",
    "      return (input_ids, masks)\n",
    "    \n",
    "    \n",
    "    def _build_model(self):\n",
    "\n",
    "      with tpu_strategy.scope():\n",
    "          config = RobertaConfig.from_pretrained(self.model_name, num_labels=2)\n",
    "          model = roBERTaModel.from_pretrained(self.model_name)\n",
    "          optimizer = tf.keras.optimizers.Adam(learning_rate=self.lr)\n",
    "          loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "          metric = tf.keras.metrics.BinaryAccuracy('accuracy')\n",
    "          model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "      return model\n",
    "    \n",
    "    \n",
    "    def train_and_predict(self, df, test):\n",
    "        model_count = self.splits\n",
    "        X_test_encoded = self.encode(test['text'])\n",
    "        y_pred = np.zeros((1, X_test_encoded[0].shape[0], 2))\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=model_count, random_state=None, shuffle=False)\n",
    "        for fold, (trn_idx, val_idx) in enumerate(skf.split(df['text'], df['target'])):\n",
    "            \n",
    "            print('\\nFold {}\\n'.format(fold))\n",
    "        \n",
    "            model = self._build_model()\n",
    "            X_trn = df.loc[trn_idx, 'text']\n",
    "            X_val = df.loc[val_idx, 'text']\n",
    "\n",
    "            X_trn_encoded = self.encode(X_trn)\n",
    "            y_trn = df.loc[trn_idx, 'target'].values.reshape(-1, 1)\n",
    "            X_val_encoded = self.encode(X_val)\n",
    "            y_val = df.loc[val_idx, 'target'].values.reshape(-1, 1)\n",
    "\n",
    "            y_trn_encoded, y_val_encoded = tf.keras.utils.to_categorical(y_trn), tf.keras.utils.to_categorical(y_val)\n",
    "\n",
    "            callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)]\n",
    "\n",
    "            history = model.fit([X_trn_encoded[0], X_trn_encoded[1]], y_trn_encoded, validation_data=([X_val_encoded[0], X_val_encoded[1]], y_val_encoded), epochs=self.epochs, batch_size=self.batch_size, callbacks=callbacks)\n",
    "            util.plot_history(history)\n",
    "\n",
    "            y_pred += model.predict(X_test_encoded)\n",
    "            del model\n",
    "            gc.collect()\n",
    "\n",
    "        return y_pred / model_count\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O96UocA-KxVl"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "roBERTa = roBERTaClassifier(max_seq_length=128, lr=3e-5, epochs=10, batch_size=128, splits=2)\n",
    "y_pred = roBERTa.train_and_predict(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EEH1WmMpctc7"
   },
   "outputs": [],
   "source": [
    "y_pred[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dqiiZRMocu1j"
   },
   "outputs": [],
   "source": [
    "final_df = pd.read_csv('sample_submission.csv')\n",
    "final_df['target'] = np.argmax(y_pred[0], axis=1).flatten()\n",
    "final_df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DhYlNugxs0rs"
   },
   "outputs": [],
   "source": [
    "final_df.to_csv('roBERTa.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "roBERTa",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
